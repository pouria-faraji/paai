version: '3'

networks:
  mongo_default:
    external: true

services:
  spark-master:
    image: bitnami/spark:3.3
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8080:8080'
      - '7077:7077'
    networks:
      - mongo_default
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  spark-worker:
    image: bitnami/spark:3.3
    networks:
      - mongo_default
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=512M
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master
    deploy:
      replicas: 0 # It will be 1 during application running.
      restart_policy:
        condition: on-failure
        delay: 10s

  spark-job:
    image: bitnami/spark:3.3
    networks:
      - mongo_default
    depends_on:
      - spark-master
      - spark-worker
    volumes:
      - ./pyspark_job.py:/pyspark_job.py
    command: spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 --master spark://spark-master:7077 --executor-memory 512M /pyspark_job.py 
    deploy:
      replicas: 0 # It will be 1 during application running.
      restart_policy:
        condition: on-failure
        delay: 10s

