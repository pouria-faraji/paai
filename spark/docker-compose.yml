version: '3'

networks:
  # default:
  mongo_default:
    external: true

services:
  spark-master:
    image: bitnami/spark:3.3
    # build:
    #   context: .
    #   dockerfile: Dockerfile
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8080:8080'
      - '7077:7077'
    networks:
      - mongo_default
      # - default
    # healthcheck:
    #   test: ["CMD", "curl", "-f",  "http://localhost:8080"]
    #   interval: 20s
    #   timeout: 20s
    #   retries: 5
    #   start_period: 10s
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  spark-worker:
    image: bitnami/spark:3.3
    networks:
      - mongo_default
      # - default
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master
      # spark-master:
      #   condition: service_healthy
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 10s

  spark-job:
    image: bitnami/spark:3.3
    networks:
      - mongo_default
      # - default
    depends_on:
      - spark-master
      - spark-worker
      # spark-master:
      #   condition: service_healthy
    volumes:
      - ./pyspark_job.py:/pyspark_job.py
    command: spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.mongodb.spark:mongo-spark-connector_2.12:10.1.1 --master spark://spark-master:7077 /pyspark_job.py 
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 10s

